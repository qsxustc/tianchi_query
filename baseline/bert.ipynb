{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 查看当前kernel下已安装的包  list packages\n",
    "# !pip list --format=columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 安装拓展包\n",
    "# !pip install dataclasses --user\n",
    "# !pip install transformers --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 下载BERT模型\n",
    "# !mkdir chinese-bert-wwm-ext\n",
    "# !wget -P chinese-bert-wwm-ext https://huggingface.co/hfl/chinese-bert-wwm-ext/resolve/main/added_tokens.json\n",
    "# !wget -P chinese-bert-wwm-ext https://huggingface.co/hfl/chinese-bert-wwm-ext/resolve/main/config.json\n",
    "# !wget -P chinese-bert-wwm-ext https://huggingface.co/hfl/chinese-bert-wwm-ext/resolve/main/pytorch_model.bin\n",
    "# !wget -P chinese-bert-wwm-ext https://huggingface.co/hfl/chinese-bert-wwm-ext/resolve/main/special_tokens_map.json\n",
    "# !wget -P chinese-bert-wwm-ext https://huggingface.co/hfl/chinese-bert-wwm-ext/resolve/main/tokenizer.json\n",
    "# !wget -P chinese-bert-wwm-ext https://huggingface.co/hfl/chinese-bert-wwm-ext/resolve/main/tokenizer_config.json\n",
    "# !wget -P chinese-bert-wwm-ext https://huggingface.co/hfl/chinese-bert-wwm-ext/resolve/main/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\25389\\.conda\\envs\\query\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import dataclasses\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Union, Any\n",
    "from collections.abc import Mapping\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import set_seed\n",
    "from transformers.data.processors.utils import InputExample, InputFeatures\n",
    "from transformers.data import DefaultDataCollator\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer, PreTrainedModel, BertForSequenceClassification\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers.optimization import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "set_seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "\n",
    "    model_dir: str = field(\n",
    "        default='../chinese-bert-wwm-ext',\n",
    "        metadata={'help': 'The pretrained model directory'}\n",
    "    )\n",
    "    data_dir: str = field(\n",
    "        default='../KUAKE-QQR',\n",
    "        metadata={'help': 'The data directory'}\n",
    "    )\n",
    "    max_length: int = field(\n",
    "        default=64,\n",
    "        metadata={'help': 'Maximum sequence length allowed to input'}\n",
    "    )\n",
    "\n",
    "    def __str__(self):\n",
    "        self_as_dict = dataclasses.asdict(self)\n",
    "        attrs_as_str = [f\"{k}={v},\\n\" for k, v in sorted(self_as_dict.items())]\n",
    "        return f\"{self.__class__.__name__}(\\n{''.join(attrs_as_str)})\"\n",
    "        \n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self), indent=2) + \"\\n\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments:\n",
    "\n",
    "    output_dir: str = field(\n",
    "        default='output_data/',\n",
    "        metadata={'help': 'The output directory where the model predictions and checkpoints will be written.'}\n",
    "    )\n",
    "    train_batch_size: int = field(\n",
    "        default=16,\n",
    "        metadata={'help': 'batch size for training'}\n",
    "    )\n",
    "    eval_batch_size: int = field(\n",
    "        default=32,\n",
    "        metadata={'help': 'batch size for evaluation'}\n",
    "    )\n",
    "    gradient_accumulation_steps: int = field(\n",
    "        default=1,\n",
    "        metadata={'help': 'Number of updates steps to accumulate before performing a backward/update pass.'}\n",
    "    )\n",
    "    num_train_epochs: int = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"The total number of training epochs\"}\n",
    "    )\n",
    "    learning_rate: float = field(\n",
    "        default=3e-5,\n",
    "        metadata={'help': '\"The initial learning rate for AdamW.'}\n",
    "    )\n",
    "    weight_decay: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"Weight decay for AdamW\"}\n",
    "    )\n",
    "    warmup_ratio: float = field(\n",
    "        default=0.1,\n",
    "        metadata={\"help\": \"Linear warmup over warmup_ratio fraction of total steps.\"}\n",
    "    )\n",
    "    dataloader_num_workers: int = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"Number of subprocesses to use for data loading (PyTorch only)\"}\n",
    "    )\n",
    "    \n",
    "    logging_steps: int = field(\n",
    "        default=100,\n",
    "        metadata={'help': 'logging states every X updates steps.'}\n",
    "    )\n",
    "    eval_steps: int = field(\n",
    "        default=250,\n",
    "        metadata={'help': 'Run an evaluation every X steps.'}\n",
    "    )\n",
    "    device: str = field(\n",
    "        default='cpu',\n",
    "        metadata={\"help\": 'The device used for training'}\n",
    "    )\n",
    "\n",
    "    def get_warmup_steps(self, num_training_steps):\n",
    "        return int(num_training_steps * self.warmup_ratio)\n",
    "\n",
    "    def __str__(self):\n",
    "        self_as_dict = dataclasses.asdict(self)\n",
    "        attrs_as_str = [f\"{k}={v},\\n\" for k, v in sorted(self_as_dict.items())]\n",
    "        return f\"{self.__class__.__name__}(\\n{''.join(attrs_as_str)})\"\n",
    "        \n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self), indent=2) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QQRProcessor:\n",
    "    TASK = 'KUAKE-QQR'\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.task_dir = os.path.join(data_dir)\n",
    "\n",
    "    def get_train_examples(self):\n",
    "        return self._create_examples(os.path.join(self.task_dir, f'{self.TASK}_train.json'))\n",
    "\n",
    "    def get_dev_examples(self):\n",
    "        return self._create_examples(os.path.join(self.task_dir, f'{self.TASK}_dev.json'))\n",
    "\n",
    "    def get_test_examples(self):\n",
    "        return self._create_examples(os.path.join(self.task_dir, f'{self.TASK}_test.json'))\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"0\", \"1\", \"2\"]\n",
    "\n",
    "    def _create_examples(self, data_path):\n",
    "\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            samples = json.load(f)\n",
    "\n",
    "        examples = []\n",
    "        for sample in samples:\n",
    "            guid = sample['id']\n",
    "            text_a = sample['query1']\n",
    "            text_b = sample['query2']\n",
    "            label = sample.get('label', None)\n",
    "\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        examples: List[InputExample],\n",
    "        label_list: List[Union[str, int]],\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_length: int = 128,\n",
    "        processor = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.examples = examples\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "\n",
    "        self.label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "        self.id2label = {idx: label for idx, label in enumerate(label_list)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index) -> InputFeatures:\n",
    "        \n",
    "        example = self.examples[index]\n",
    "        label = None\n",
    "        # if example.label is not None:\n",
    "        #     label = self.label2id[example.label]\n",
    "\n",
    "        if example.label is None or example.label == '':\n",
    "            label = 0  # 测试集默认标签\n",
    "        else:\n",
    "            if example.label == 'NA':\n",
    "                example.label = '0'\n",
    "            label = self.label2id[example.label]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text=example.text_a,\n",
    "            text_pair=example.text_b,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        feature = InputFeatures(**inputs, label=label)\n",
    "\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer_and_scheduler(\n",
    "    args: TrainingArguments,\n",
    "    model: PreTrainedModel,\n",
    "    num_training_steps: int,\n",
    "):\n",
    "    decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters, \n",
    "        lr=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_training_steps=num_training_steps, \n",
    "        num_warmup_steps=args.get_warmup_steps(num_training_steps)\n",
    "    )\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_input(data: Union[torch.Tensor, Any], device: str = 'cuda'):\n",
    "    \"\"\"\n",
    "    Prepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.\n",
    "    \"\"\"\n",
    "    if isinstance(data, Mapping):\n",
    "        return type(data)({k: _prepare_input(v, device) for k, v in data.items()})\n",
    "    elif isinstance(data, (tuple, list)):\n",
    "        return type(data)(_prepare_input(v, device) for v in data)\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        kwargs = dict(device=device)\n",
    "        return data.to(**kwargs)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def evaluate(\n",
    "    args: TrainingArguments,\n",
    "    model: PreTrainedModel,\n",
    "    eval_dataloader\n",
    "):\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for item in eval_dataloader:\n",
    "        inputs = _prepare_input(item, device=args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, return_dict=True)\n",
    "            loss = outputs.loss\n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "\n",
    "            preds = torch.argmax(outputs.logits.cpu(), dim=-1).numpy()\n",
    "            preds_list.append(preds)\n",
    "\n",
    "            labels_list.append(inputs['labels'].cpu().numpy())\n",
    "    \n",
    "    preds = np.concatenate(preds_list, axis=0)\n",
    "    labels = np.concatenate(labels_list, axis=0)\n",
    "    loss = np.mean(loss_list)\n",
    "    accuracy = simple_accuracy(preds, labels)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    args: TrainingArguments,\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    train_dataset,\n",
    "    dev_dataset,\n",
    "    data_collator,\n",
    "):\n",
    "\n",
    "    # initialize dataloader\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_dataset, \n",
    "        batch_size=args.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    dev_dataloader = DataLoader(\n",
    "        dataset=dev_dataset,\n",
    "        batch_size=args.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    num_examples = len(train_dataloader.dataset)\n",
    "    total_train_batch_size = args.gradient_accumulation_steps * args.train_batch_size\n",
    "    num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps\n",
    "    num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "    \n",
    "    max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n",
    "    num_train_epochs = math.ceil(args.num_train_epochs)\n",
    "    num_train_samples = len(train_dataset) * args.num_train_epochs\n",
    "\n",
    "    optimizer, lr_scheduler = create_optimizer_and_scheduler(\n",
    "        args, model, num_training_steps=max_steps\n",
    "    )\n",
    "\n",
    "    print(\"***** Running training *****\")\n",
    "    print(f\"  Num examples = {num_examples}\")\n",
    "    print(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "    print(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "    print(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size}\")\n",
    "    print(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    print(f\"  Total optimization steps = {max_steps}\")\n",
    "\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    t_loss = 0.0\n",
    "    global_steps = 0\n",
    "\n",
    "    best_metric = 0.0\n",
    "    best_steps = -1\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        for step, item in enumerate(train_dataloader):\n",
    "            inputs = _prepare_input(item, device=args.device)\n",
    "            outputs = model(**inputs, return_dict=True)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            if args.gradient_accumulation_steps > 0:\n",
    "                loss /= args.gradient_accumulation_steps\n",
    "            \n",
    "            loss.backward()\n",
    "            t_loss += loss.detach()\n",
    "\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                model.zero_grad()\n",
    "                global_steps += 1\n",
    "\n",
    "                if global_steps % args.logging_steps == 0:\n",
    "                    print(f'Training: Epoch {epoch + 1}/{num_train_epochs} - Step {(step + 1) // args.gradient_accumulation_steps} - Loss {t_loss}')\n",
    "\n",
    "                t_loss = 0.0\n",
    "\n",
    "            if (global_steps + 1) % args.eval_steps == 0:\n",
    "                \n",
    "                loss, acc = evaluate(args, model, dev_dataloader)\n",
    "                print(f'Evaluation: Epoch {epoch + 1}/{num_train_epochs} - Step {(global_steps + 1) // args.gradient_accumulation_steps} - Loss {loss} - Accuracy {acc}')\n",
    "\n",
    "                if acc > best_metric:\n",
    "                    best_metric = acc\n",
    "                    best_steps = global_steps\n",
    "                    \n",
    "                    saved_dir = os.path.join(args.output_dir, f'checkpoint-{best_steps}')\n",
    "                    os.makedirs(saved_dir, exist_ok=True)\n",
    "                    model.save_pretrained(saved_dir)\n",
    "                    tokenizer.save_vocabulary(save_directory=saved_dir)\n",
    "\n",
    "    return best_steps, best_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    args: TrainingArguments,\n",
    "    model: PreTrainedModel,\n",
    "    test_dataset,\n",
    "    data_collator\n",
    "):\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=args.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    print(\"***** Running prediction *****\")\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for item in test_dataloader:\n",
    "        inputs = _prepare_input(item, device=args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, return_dict=True)\n",
    "\n",
    "            preds = torch.argmax(outputs.logits.cpu(), dim=-1).numpy()\n",
    "            preds_list.append(preds)\n",
    "\n",
    "    print(f'Prediction Finished!')\n",
    "    preds = np.concatenate(preds_list, axis=0).tolist()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "def generate_commit(output_dir, task_name, test_dataset, preds: List[int]):\n",
    "\n",
    "    test_examples = test_dataset.examples\n",
    "    pred_test_examples = []\n",
    "    for idx in range(len(test_examples)):\n",
    "        example = test_examples[idx]\n",
    "        label  = test_dataset.id2label[preds[idx]]\n",
    "        pred_example = {'id': example.guid, 'query1': example.text_a, 'query2': example.text_b, 'label': label}\n",
    "        pred_test_examples.append(pred_example)\n",
    "    \n",
    "    with open(os.path.join(output_dir, f'{task_name}_test.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(pred_test_examples, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataTrainingArguments(\n",
      "data_dir=../KUAKE-QQR,\n",
      "max_length=64,\n",
      "model_dir=../chinese-bert-wwm-ext,\n",
      ")\n",
      "TrainingArguments(\n",
      "dataloader_num_workers=0,\n",
      "device=cpu,\n",
      "eval_batch_size=32,\n",
      "eval_steps=250,\n",
      "gradient_accumulation_steps=1,\n",
      "learning_rate=3e-05,\n",
      "logging_steps=100,\n",
      "num_train_epochs=3,\n",
      "output_dir=output_data/,\n",
      "train_batch_size=16,\n",
      "warmup_ratio=0.1,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\25389\\.conda\\envs\\query\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2814\n",
      "Training: Epoch 1/3 - Step 100 - Loss 0.5527257323265076\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "data_args = DataTrainingArguments()\n",
    "training_args = TrainingArguments()\n",
    "print(data_args)\n",
    "print(training_args)\n",
    "\n",
    "# initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(data_args.model_dir)\n",
    "\n",
    "# initialize dataset\n",
    "processor = QQRProcessor(data_args.data_dir)\n",
    "train_dataset = ClassificationDataset(\n",
    "    processor.get_train_examples(),\n",
    "    label_list=processor.get_labels(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=data_args.max_length,\n",
    ")\n",
    "dev_dataset = ClassificationDataset(\n",
    "    processor.get_dev_examples(),\n",
    "    label_list=processor.get_labels(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=data_args.max_length,\n",
    ")\n",
    "test_dataset = ClassificationDataset(\n",
    "    processor.get_test_examples(),\n",
    "    label_list=processor.get_labels(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=data_args.max_length,\n",
    ")\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "model_name = f'{os.path.split(data_args.model_dir)[-1]}-{str(int(time.time()))}'\n",
    "training_args.output_dir = os.path.join(training_args.output_dir, model_name)\n",
    "if not os.path.exists(training_args.output_dir):\n",
    "    os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(data_args.model_dir, num_labels=len(processor.get_labels()))\n",
    "model.to(training_args.device)\n",
    "\n",
    "best_steps, best_metric = train(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dev_dataset=dev_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(f'Training Finished! Best step - {best_steps} - Best accuracy {best_metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model_dir = os.path.join(training_args.output_dir, f'checkpoint-{best_steps}')\n",
    "model = BertForSequenceClassification.from_pretrained(best_model_dir, num_labels=len(processor.get_labels()))\n",
    "model.to(training_args.device)\n",
    "\n",
    "model.save_pretrained(training_args.output_dir)\n",
    "torch.save(training_args, os.path.join(training_args.output_dir, 'training_args.bin'))\n",
    "tokenizer.save_vocabulary(save_directory=training_args.output_dir)\n",
    "\n",
    "preds = predict(training_args, model, test_dataset, data_collator)\n",
    "generate_commit(data_args.output_dir, processor.TASK, test_dataset, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "query",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [],
   "description": "",
   "notebookId": "391766",
   "source": "dsw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
